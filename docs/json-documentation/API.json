[
{"description":"Build a frequency hashmap where\n- the keys are the entries in `tokens`\n- the values are the frequency of each entry (`tokens`)","tags":[{"title":"param","description":"Normalized word array","type":{"type":"NameExpression","name":"Array"},"name":"tokens"},{"title":"returns","description":"FrequencyTable","type":{"type":"NameExpression","name":"Object"}},{"title":"name","name":"FrequencyTable"},{"title":"kind","kind":"function"},{"title":"memberof","description":"NaiveBayesClassifier"},{"title":"instance"}],"context":{"loc":{"start":{"line":141,"column":0},"end":{"line":153,"column":2}},"file":"/Users/hsalem/Developer/naive-bayes-classifier/src/NaiveBayesClassifier.js","code":"'use strict';\n\n/**\n * NaiveBayesClassifier constructor fuction. Takes an (optional) options object containing:\n *   - {Function} tokenizer => custom tokenization function\n *\n * @constructor\n * @param  {Object} options - Options to be used for intialisation\n * @return {Object} NaiveBayesClassifier\n */\nvar NaiveBayesClassifier = function(options) {\n\t// OPTIONS\n\t// =============================================================================\n\tthis.options = {};\n\n\tif (!options) {\n\t\tif (typeof options !== 'object') {\n\t\t\tthrow new TypeError('NaiveBayesClassifier got invalid `options`: `' + options + '`. Please pass in an object.');\n\t\t}\n\t\tthis.options = options;\n\t}\n\n\t// TOKENIZER\n\t// =============================================================================\n\t/**\n\t * Given an input string, tokenize it into an array of word tokens.\n\t * This tokenizer adopts a naive \"Bag of words\" assumption.\n\t * This is the default tokenization function used if the user does not provide one in `options`.\n\t *\n\t * @private\n\t * @param  {String} text\n\t * @return {Array}\n\t */\n\tvar defaultTokenizer = function(text) {\n\t\t//remove punctuation from text (anything that isn't a word char or a space), and enforce lowercase\n\t\tvar rgxPunctuation = new RegExp(/[^\\w\\s]/g);\n\t\tvar sanitized = text.replace(rgxPunctuation, ' ').toLowerCase();\n\n\t\treturn sanitized.split(/\\s+/);\n\t};\n\n\tthis.tokenizer = this.options.tokenizer || defaultTokenizer;\n\t\n\t// VOCABULARY\n\t// =============================================================================\n\t//initialize our vocabulary and its size\n\tthis.vocabulary = {};\n\tthis.vocabularySize = 0;\n\n\t// CATEGORIES - hashmap of our category names\n\t// =============================================================================\n\tthis.categories = {};\n\n\t// PRIOR PROBABILITY: P(Cj) = docCount(C=cj) / Ndoc\n\t// =============================================================================\n\n\t//document frequency table for each of our categories\n\t//=> for each category, how often were documents mapped to it\n\tthis.docFrequencyCount = {}; //docCount(class)\n\n\tthis.totalNumberOfDocuments = 0; //Ndoc => number of documents we have learned from\n\n\t// LIKELIHOOD: P(wi|Cj) = count(wi,cj) / SUM[(for w in v) count(w,cj)]\n\t// =============================================================================\n\n\t//word frequency table for each of our categories\n\t//=> for each category, how frequent was a given word mapped to it\n\tthis.wordFrequencyCount = {}; //count(wi,cj)\n\n\t//word count table for each of our categories\n\t//=> for each category, how many words in total were mapped to it\n\tthis.wordCount = {}; //SUM[(for w in v) count(w,cj)\n};\n\n/**\n * Library version number\n */\nNaiveBayesClassifier.VERSION = '0.1.0'; // current version | Note: JS Functions are first class Objects\n\nNaiveBayesClassifier.withClassifier = function(classifier) {\n\treturn new NaiveBayesClassifier(classifier.options);\n};\n\nNaiveBayesClassifier.prototype.addWordToVocabulary = function(word) {\n\tif (!this.vocabulary[word]) {\n\t\tthis.vocabulary[word] = true;\n\t\tthis.vocabularySize += 1;\n\t}\n};\nNaiveBayesClassifier.prototype.getVocabularySize = function() {\n\tthis.vocabularySize = Object.keys(this.vocabulary).length;\n\treturn this.vocabularySize;\n};\n\n/**\n * Initialize each of our data structure entries for this new category\n *\n * @param  {String} categoryName - Name of the category you want to get or create\n * @return {String} category\n */\nNaiveBayesClassifier.prototype.getOrCreateCategory = function(categoryName) {\n\tif (!categoryName || typeof categoryName !== 'string') {\n\t\tthrow new TypeError('Category creator got invalid category name: `' + categoryName + '`. Please pass in a String.');\n\t}\n\n\t//simple singleton for each category\n\tif (!this.categories[categoryName]) {\n\t\t//setup counters\n\t\tthis.docFrequencyCount[categoryName] = 0;\n\t\tthis.wordFrequencyCount[categoryName] = {};\n\t\tthis.wordCount[categoryName] = 0;\n\n\t\t//add new category to our list\n\t\tthis.categories[categoryName] = true;\n\t}\n\treturn this.categories[categoryName] ? categoryName : undefined;\n};\n\n/**\n * Build a frequency hashmap where\n * - the keys are the entries in `tokens`\n * - the values are the frequency of each entry (`tokens`)\n *\n * @param  {Array} tokens - Normalized word array\n * @return {Object} FrequencyTable\n */\nNaiveBayesClassifier.prototype.FrequencyTable = function(tokens) {\n\tvar frequencyTable = {};\n\n\ttokens.forEach(function (token) {\n\t\tif (!frequencyTable[token]) {\n\t\t\tfrequencyTable[token] = 1;\n\t\t} else {\n\t\t\tfrequencyTable[token] += 1;\n\t\t}\n\t});\n\n\treturn frequencyTable;\n};\n\n/**\n * Train our naive-bayes classifier by telling it what `category` some `text` corresponds to.\n *\n * @param  {String} text\n * @param  {String} category\n * @return {Object} NaiveBayesClassifier\n */\nNaiveBayesClassifier.prototype.learn = function(text, category) {\n\tvar self = this; //get reference to instance\n\n\tcategory = self.Category(category); //get or create a category\n\n\tself.docFrequencyCount[category] += 1; //update our count of how many documents mapped to this category\n\tself.totalNumberOfDocuments += 1; //update the total number of documents we have learned from\n\n\tvar tokens = self.tokenizer(text); //break up the text into tokens\n\tvar tokenFrequencyTable = self.FrequencyTable(tokens); //get a frequency count for each token in the text\n\n\t// Update our vocabulary and our word frequency counts for this category\n\t// =============================================================================\n\tObject\n\t.keys(tokenFrequencyTable)\n\t.forEach(function(token) { //for each token in our tokenFrequencyTable\n\t\t\n\t\tself.addWordToVocabulary(token); //add this word to our vocabulary if not already existing\n\n\t\tvar frequencyOfTokenInText = tokenFrequencyTable[token]; //look it up once, for speed\n\n\t\t//update the frequency information for this word in this category\n\t\tif (!self.wordFrequencyCount[category][token]) {\n\t\t\tself.wordFrequencyCount[category][token] = frequencyOfTokenInText; //set it for the first time\n\t\t} else {\n\t\t\tself.wordFrequencyCount[category][token] += frequencyOfTokenInText; //add to what's already there in the count\n\t\t}\n\n\t\tself.wordCount[category] += frequencyOfTokenInText; //add to the count of all words we have seen mapped to this category\n\t});\n\n\treturn self;\n};\n\n/**\n * Calculate probability that a `token` belongs to a `category`\n *\n * @param  {String} token\n * @param  {String} category\n * @return {Number} probability\n */\nNaiveBayesClassifier.prototype.tokenProbability = function(token, category) {\n\t// Recall => P(wi|Cj) = count(wi,cj) / SUM[(for w in v) count(w,cj)]\n\t// =============================================================================\n\n\t//how many times this word has occurred in documents mapped to this category\n\tvar wordFrequencyCount = this.wordFrequencyCount[category][token] || 0; //count(wi,cj)\n\n\t//what is the count of all words that have ever been mapped to this category\n\tvar wordCount = this.wordCount[category]; //SUM[(for w in v) count(w,cj)\n\n\t//use laplace Add-1 Smoothing equation\n\t//=> ( P(wi|Cj) = count(wi,cj) + 1 ) / ( SUM[(for w in v) count(w,cj)] + |VocabSize| )\n\treturn ( wordFrequencyCount + 1 ) / ( wordCount + this.getVocabularySize() );\n};\n\n/**\n * Determine what category `text` belongs to.\n * Use Laplace (add-1) smoothing to adjust for words that do not appear in our vocabulary (unknown words)\n *\n * @param  {String} text\n * @return {Object} cMAP and categories\n */\nNaiveBayesClassifier.prototype.categorize = function (text) {\n\tvar self = this,  //get reference to instance\n\t\t\tmaxProbability = -Infinity,\n\t\t\ttotalProbabilities = 0,\n\t\t\tcMAP = {}, //category of “maximum a posteriori” => most likely category\n\t\t\tcategoryProbabilities = {}; //probabilities of all categories\n\n\tvar tokens = self.tokenizer(text),\n\t\ttokenFrequencyTable = self.FrequencyTable(tokens);\n\n\tObject\n\t.keys(self.categories)\n\t.forEach(function(category) { //for each category, find the probability of the text belonging to it\n\t\tif (!self.categories[category]) { return; } //ignore categories that have been switched off\n\n\t\t// 1. Find overall probability of this category\n\t\t//=> P(Cj) = docCount(C=cj) / Ndoc\n\t\t// =============================================================================\n\t\t\n\t\t//Put of all documents we've ever looked at, how many were mapped to this category\n\t\tvar categoryProbability = self.docFrequencyCount[category] / self.totalNumberOfDocuments;\n\n\t\t//take the log to avoid underflow with large datasets - http://www.johndcook.com/blog/2012/07/26/avoiding-underflow-in-bayesian-computations/\n\t\tvar logCategoryProbability = Math.log(categoryProbability); //start with P(Cj), we will add P(wi|Cj) incrementally below\n\n\t\t// 2. Find probability of each word in this categogy\n\t\t//=> P(wi|Cj) = count(wi,cj) / SUM[(for w in v) count(w,cj)]\n\t\t// =============================================================================\n\n\t\tObject\n\t\t.keys(tokenFrequencyTable)\n\t\t.forEach(function(token) { //for each token in our token frequency table\n\n\t\t\t//determine the log of the probability of this token belonging to the current category\n\t\t\t//=> log( P(w|c) )\n\t\t\tvar tokenProbability = self.tokenProbability(token, category);\n\t\t\t//and add it to our running probability that the text belongs to the current category\n\t\t\tlogCategoryProbability += Math.log(tokenProbability); //TODO: look into *frequencyTable[token];\n\n\t\t\t// console.log('token: %s | category: `%s` | probability: %d', token, category, tokenProbability);\n\t\t});\n\n\t\t// 3. Find the most likely category, thus far...\n\t\t// =============================================================================\n\n\t\tcategoryProbability = Math.exp(logCategoryProbability); //reverse the log and get an actual value\n\t\ttotalProbabilities += categoryProbability; //calculate totals as we go, we'll use this to normalise later\n\n\t\tif (logCategoryProbability > maxProbability) { //find cMAP\n\t\t\tmaxProbability = logCategoryProbability;\n\t\t\tcMAP = {\n\t\t\t\tcategory: category,\n\t\t\t\tprobability: categoryProbability\n\t\t\t};\n\t\t}\n\n\t\tcategoryProbabilities[category] = categoryProbability;\n\t});\n\n\t//normalise (out of 1) the probabilities, so that they make a bit more sense to the average person\n\tObject\n\t.keys(categoryProbabilities)\n\t.forEach(function(category) {\n\t\tcategoryProbabilities[category] /= totalProbabilities;\n\t});\n\n\treturn {\n\t\tcategory: cMAP.category || 'unclassified',\n\t\tprobability: (cMAP.probability /= totalProbabilities) || -Infinity,\n\t\tcategories: categoryProbabilities\n\t};\n};"},"params":[{"title":"param","description":"Normalized word array","type":{"type":"NameExpression","name":"Array"},"name":"tokens"}],"returns":[{"title":"returns","description":"FrequencyTable","type":{"type":"NameExpression","name":"Object"}}],"name":"FrequencyTable","kind":"function","memberof":"NaiveBayesClassifier","scope":"instance"}
,
{"description":"NaiveBayesClassifier constructor fuction. Takes an (optional) options object containing:\n  - {Function} tokenizer => custom tokenization function","tags":[{"title":"class","description":null,"type":null,"name":null},{"title":"param","description":"Options to be used for intialisation","type":{"type":"NameExpression","name":"Object"},"name":"options"},{"title":"returns","description":"NaiveBayesClassifier","type":{"type":"NameExpression","name":"Object"}},{"title":"name","name":"NaiveBayesClassifier"},{"title":"kind","kind":"function"}],"context":{"loc":{"start":{"line":25,"column":0},"end":{"line":87,"column":2}},"file":"/Users/hsalem/Developer/naive-bayes-classifier/src/NaiveBayesClassifier.js","code":"'use strict';\n\n/**\n * NaiveBayesClassifier constructor fuction. Takes an (optional) options object containing:\n *   - {Function} tokenizer => custom tokenization function\n *\n * @constructor\n * @param  {Object} options - Options to be used for intialisation\n * @return {Object} NaiveBayesClassifier\n */\nvar NaiveBayesClassifier = function(options) {\n\t// OPTIONS\n\t// =============================================================================\n\tthis.options = {};\n\n\tif (!options) {\n\t\tif (typeof options !== 'object') {\n\t\t\tthrow new TypeError('NaiveBayesClassifier got invalid `options`: `' + options + '`. Please pass in an object.');\n\t\t}\n\t\tthis.options = options;\n\t}\n\n\t// TOKENIZER\n\t// =============================================================================\n\t/**\n\t * Given an input string, tokenize it into an array of word tokens.\n\t * This tokenizer adopts a naive \"Bag of words\" assumption.\n\t * This is the default tokenization function used if the user does not provide one in `options`.\n\t *\n\t * @private\n\t * @param  {String} text\n\t * @return {Array}\n\t */\n\tvar defaultTokenizer = function(text) {\n\t\t//remove punctuation from text (anything that isn't a word char or a space), and enforce lowercase\n\t\tvar rgxPunctuation = new RegExp(/[^\\w\\s]/g);\n\t\tvar sanitized = text.replace(rgxPunctuation, ' ').toLowerCase();\n\n\t\treturn sanitized.split(/\\s+/);\n\t};\n\n\tthis.tokenizer = this.options.tokenizer || defaultTokenizer;\n\t\n\t// VOCABULARY\n\t// =============================================================================\n\t//initialize our vocabulary and its size\n\tthis.vocabulary = {};\n\tthis.vocabularySize = 0;\n\n\t// CATEGORIES - hashmap of our category names\n\t// =============================================================================\n\tthis.categories = {};\n\n\t// PRIOR PROBABILITY: P(Cj) = docCount(C=cj) / Ndoc\n\t// =============================================================================\n\n\t//document frequency table for each of our categories\n\t//=> for each category, how often were documents mapped to it\n\tthis.docFrequencyCount = {}; //docCount(class)\n\n\tthis.totalNumberOfDocuments = 0; //Ndoc => number of documents we have learned from\n\n\t// LIKELIHOOD: P(wi|Cj) = count(wi,cj) / SUM[(for w in v) count(w,cj)]\n\t// =============================================================================\n\n\t//word frequency table for each of our categories\n\t//=> for each category, how frequent was a given word mapped to it\n\tthis.wordFrequencyCount = {}; //count(wi,cj)\n\n\t//word count table for each of our categories\n\t//=> for each category, how many words in total were mapped to it\n\tthis.wordCount = {}; //SUM[(for w in v) count(w,cj)\n};\n\n/**\n * Library version number\n */\nNaiveBayesClassifier.VERSION = '0.1.0'; // current version | Note: JS Functions are first class Objects\n\nNaiveBayesClassifier.withClassifier = function(classifier) {\n\treturn new NaiveBayesClassifier(classifier.options);\n};\n\nNaiveBayesClassifier.prototype.addWordToVocabulary = function(word) {\n\tif (!this.vocabulary[word]) {\n\t\tthis.vocabulary[word] = true;\n\t\tthis.vocabularySize += 1;\n\t}\n};\nNaiveBayesClassifier.prototype.getVocabularySize = function() {\n\tthis.vocabularySize = Object.keys(this.vocabulary).length;\n\treturn this.vocabularySize;\n};\n\n/**\n * Initialize each of our data structure entries for this new category\n *\n * @param  {String} categoryName - Name of the category you want to get or create\n * @return {String} category\n */\nNaiveBayesClassifier.prototype.getOrCreateCategory = function(categoryName) {\n\tif (!categoryName || typeof categoryName !== 'string') {\n\t\tthrow new TypeError('Category creator got invalid category name: `' + categoryName + '`. Please pass in a String.');\n\t}\n\n\t//simple singleton for each category\n\tif (!this.categories[categoryName]) {\n\t\t//setup counters\n\t\tthis.docFrequencyCount[categoryName] = 0;\n\t\tthis.wordFrequencyCount[categoryName] = {};\n\t\tthis.wordCount[categoryName] = 0;\n\n\t\t//add new category to our list\n\t\tthis.categories[categoryName] = true;\n\t}\n\treturn this.categories[categoryName] ? categoryName : undefined;\n};\n\n/**\n * Build a frequency hashmap where\n * - the keys are the entries in `tokens`\n * - the values are the frequency of each entry (`tokens`)\n *\n * @param  {Array} tokens - Normalized word array\n * @return {Object} FrequencyTable\n */\nNaiveBayesClassifier.prototype.FrequencyTable = function(tokens) {\n\tvar frequencyTable = {};\n\n\ttokens.forEach(function (token) {\n\t\tif (!frequencyTable[token]) {\n\t\t\tfrequencyTable[token] = 1;\n\t\t} else {\n\t\t\tfrequencyTable[token] += 1;\n\t\t}\n\t});\n\n\treturn frequencyTable;\n};\n\n/**\n * Train our naive-bayes classifier by telling it what `category` some `text` corresponds to.\n *\n * @param  {String} text\n * @param  {String} category\n * @return {Object} NaiveBayesClassifier\n */\nNaiveBayesClassifier.prototype.learn = function(text, category) {\n\tvar self = this; //get reference to instance\n\n\tcategory = self.Category(category); //get or create a category\n\n\tself.docFrequencyCount[category] += 1; //update our count of how many documents mapped to this category\n\tself.totalNumberOfDocuments += 1; //update the total number of documents we have learned from\n\n\tvar tokens = self.tokenizer(text); //break up the text into tokens\n\tvar tokenFrequencyTable = self.FrequencyTable(tokens); //get a frequency count for each token in the text\n\n\t// Update our vocabulary and our word frequency counts for this category\n\t// =============================================================================\n\tObject\n\t.keys(tokenFrequencyTable)\n\t.forEach(function(token) { //for each token in our tokenFrequencyTable\n\t\t\n\t\tself.addWordToVocabulary(token); //add this word to our vocabulary if not already existing\n\n\t\tvar frequencyOfTokenInText = tokenFrequencyTable[token]; //look it up once, for speed\n\n\t\t//update the frequency information for this word in this category\n\t\tif (!self.wordFrequencyCount[category][token]) {\n\t\t\tself.wordFrequencyCount[category][token] = frequencyOfTokenInText; //set it for the first time\n\t\t} else {\n\t\t\tself.wordFrequencyCount[category][token] += frequencyOfTokenInText; //add to what's already there in the count\n\t\t}\n\n\t\tself.wordCount[category] += frequencyOfTokenInText; //add to the count of all words we have seen mapped to this category\n\t});\n\n\treturn self;\n};\n\n/**\n * Calculate probability that a `token` belongs to a `category`\n *\n * @param  {String} token\n * @param  {String} category\n * @return {Number} probability\n */\nNaiveBayesClassifier.prototype.tokenProbability = function(token, category) {\n\t// Recall => P(wi|Cj) = count(wi,cj) / SUM[(for w in v) count(w,cj)]\n\t// =============================================================================\n\n\t//how many times this word has occurred in documents mapped to this category\n\tvar wordFrequencyCount = this.wordFrequencyCount[category][token] || 0; //count(wi,cj)\n\n\t//what is the count of all words that have ever been mapped to this category\n\tvar wordCount = this.wordCount[category]; //SUM[(for w in v) count(w,cj)\n\n\t//use laplace Add-1 Smoothing equation\n\t//=> ( P(wi|Cj) = count(wi,cj) + 1 ) / ( SUM[(for w in v) count(w,cj)] + |VocabSize| )\n\treturn ( wordFrequencyCount + 1 ) / ( wordCount + this.getVocabularySize() );\n};\n\n/**\n * Determine what category `text` belongs to.\n * Use Laplace (add-1) smoothing to adjust for words that do not appear in our vocabulary (unknown words)\n *\n * @param  {String} text\n * @return {Object} cMAP and categories\n */\nNaiveBayesClassifier.prototype.categorize = function (text) {\n\tvar self = this,  //get reference to instance\n\t\t\tmaxProbability = -Infinity,\n\t\t\ttotalProbabilities = 0,\n\t\t\tcMAP = {}, //category of “maximum a posteriori” => most likely category\n\t\t\tcategoryProbabilities = {}; //probabilities of all categories\n\n\tvar tokens = self.tokenizer(text),\n\t\ttokenFrequencyTable = self.FrequencyTable(tokens);\n\n\tObject\n\t.keys(self.categories)\n\t.forEach(function(category) { //for each category, find the probability of the text belonging to it\n\t\tif (!self.categories[category]) { return; } //ignore categories that have been switched off\n\n\t\t// 1. Find overall probability of this category\n\t\t//=> P(Cj) = docCount(C=cj) / Ndoc\n\t\t// =============================================================================\n\t\t\n\t\t//Put of all documents we've ever looked at, how many were mapped to this category\n\t\tvar categoryProbability = self.docFrequencyCount[category] / self.totalNumberOfDocuments;\n\n\t\t//take the log to avoid underflow with large datasets - http://www.johndcook.com/blog/2012/07/26/avoiding-underflow-in-bayesian-computations/\n\t\tvar logCategoryProbability = Math.log(categoryProbability); //start with P(Cj), we will add P(wi|Cj) incrementally below\n\n\t\t// 2. Find probability of each word in this categogy\n\t\t//=> P(wi|Cj) = count(wi,cj) / SUM[(for w in v) count(w,cj)]\n\t\t// =============================================================================\n\n\t\tObject\n\t\t.keys(tokenFrequencyTable)\n\t\t.forEach(function(token) { //for each token in our token frequency table\n\n\t\t\t//determine the log of the probability of this token belonging to the current category\n\t\t\t//=> log( P(w|c) )\n\t\t\tvar tokenProbability = self.tokenProbability(token, category);\n\t\t\t//and add it to our running probability that the text belongs to the current category\n\t\t\tlogCategoryProbability += Math.log(tokenProbability); //TODO: look into *frequencyTable[token];\n\n\t\t\t// console.log('token: %s | category: `%s` | probability: %d', token, category, tokenProbability);\n\t\t});\n\n\t\t// 3. Find the most likely category, thus far...\n\t\t// =============================================================================\n\n\t\tcategoryProbability = Math.exp(logCategoryProbability); //reverse the log and get an actual value\n\t\ttotalProbabilities += categoryProbability; //calculate totals as we go, we'll use this to normalise later\n\n\t\tif (logCategoryProbability > maxProbability) { //find cMAP\n\t\t\tmaxProbability = logCategoryProbability;\n\t\t\tcMAP = {\n\t\t\t\tcategory: category,\n\t\t\t\tprobability: categoryProbability\n\t\t\t};\n\t\t}\n\n\t\tcategoryProbabilities[category] = categoryProbability;\n\t});\n\n\t//normalise (out of 1) the probabilities, so that they make a bit more sense to the average person\n\tObject\n\t.keys(categoryProbabilities)\n\t.forEach(function(category) {\n\t\tcategoryProbabilities[category] /= totalProbabilities;\n\t});\n\n\treturn {\n\t\tcategory: cMAP.category || 'unclassified',\n\t\tprobability: (cMAP.probability /= totalProbabilities) || -Infinity,\n\t\tcategories: categoryProbabilities\n\t};\n};"},"params":[{"title":"param","description":"Options to be used for intialisation","type":{"type":"NameExpression","name":"Object"},"name":"options"}],"returns":[{"title":"returns","description":"NaiveBayesClassifier","type":{"type":"NameExpression","name":"Object"}}],"name":"NaiveBayesClassifier","kind":"function"}
,
{"description":"Library version number","tags":[{"title":"name","name":"VERSION"},{"title":"memberof","description":"NaiveBayesClassifier"},{"title":"static"}],"context":{"loc":{"start":{"line":92,"column":0},"end":{"line":92,"column":39}},"file":"/Users/hsalem/Developer/naive-bayes-classifier/src/NaiveBayesClassifier.js","code":"'use strict';\n\n/**\n * NaiveBayesClassifier constructor fuction. Takes an (optional) options object containing:\n *   - {Function} tokenizer => custom tokenization function\n *\n * @constructor\n * @param  {Object} options - Options to be used for intialisation\n * @return {Object} NaiveBayesClassifier\n */\nvar NaiveBayesClassifier = function(options) {\n\t// OPTIONS\n\t// =============================================================================\n\tthis.options = {};\n\n\tif (!options) {\n\t\tif (typeof options !== 'object') {\n\t\t\tthrow new TypeError('NaiveBayesClassifier got invalid `options`: `' + options + '`. Please pass in an object.');\n\t\t}\n\t\tthis.options = options;\n\t}\n\n\t// TOKENIZER\n\t// =============================================================================\n\t/**\n\t * Given an input string, tokenize it into an array of word tokens.\n\t * This tokenizer adopts a naive \"Bag of words\" assumption.\n\t * This is the default tokenization function used if the user does not provide one in `options`.\n\t *\n\t * @private\n\t * @param  {String} text\n\t * @return {Array}\n\t */\n\tvar defaultTokenizer = function(text) {\n\t\t//remove punctuation from text (anything that isn't a word char or a space), and enforce lowercase\n\t\tvar rgxPunctuation = new RegExp(/[^\\w\\s]/g);\n\t\tvar sanitized = text.replace(rgxPunctuation, ' ').toLowerCase();\n\n\t\treturn sanitized.split(/\\s+/);\n\t};\n\n\tthis.tokenizer = this.options.tokenizer || defaultTokenizer;\n\t\n\t// VOCABULARY\n\t// =============================================================================\n\t//initialize our vocabulary and its size\n\tthis.vocabulary = {};\n\tthis.vocabularySize = 0;\n\n\t// CATEGORIES - hashmap of our category names\n\t// =============================================================================\n\tthis.categories = {};\n\n\t// PRIOR PROBABILITY: P(Cj) = docCount(C=cj) / Ndoc\n\t// =============================================================================\n\n\t//document frequency table for each of our categories\n\t//=> for each category, how often were documents mapped to it\n\tthis.docFrequencyCount = {}; //docCount(class)\n\n\tthis.totalNumberOfDocuments = 0; //Ndoc => number of documents we have learned from\n\n\t// LIKELIHOOD: P(wi|Cj) = count(wi,cj) / SUM[(for w in v) count(w,cj)]\n\t// =============================================================================\n\n\t//word frequency table for each of our categories\n\t//=> for each category, how frequent was a given word mapped to it\n\tthis.wordFrequencyCount = {}; //count(wi,cj)\n\n\t//word count table for each of our categories\n\t//=> for each category, how many words in total were mapped to it\n\tthis.wordCount = {}; //SUM[(for w in v) count(w,cj)\n};\n\n/**\n * Library version number\n */\nNaiveBayesClassifier.VERSION = '0.1.0'; // current version | Note: JS Functions are first class Objects\n\nNaiveBayesClassifier.withClassifier = function(classifier) {\n\treturn new NaiveBayesClassifier(classifier.options);\n};\n\nNaiveBayesClassifier.prototype.addWordToVocabulary = function(word) {\n\tif (!this.vocabulary[word]) {\n\t\tthis.vocabulary[word] = true;\n\t\tthis.vocabularySize += 1;\n\t}\n};\nNaiveBayesClassifier.prototype.getVocabularySize = function() {\n\tthis.vocabularySize = Object.keys(this.vocabulary).length;\n\treturn this.vocabularySize;\n};\n\n/**\n * Initialize each of our data structure entries for this new category\n *\n * @param  {String} categoryName - Name of the category you want to get or create\n * @return {String} category\n */\nNaiveBayesClassifier.prototype.getOrCreateCategory = function(categoryName) {\n\tif (!categoryName || typeof categoryName !== 'string') {\n\t\tthrow new TypeError('Category creator got invalid category name: `' + categoryName + '`. Please pass in a String.');\n\t}\n\n\t//simple singleton for each category\n\tif (!this.categories[categoryName]) {\n\t\t//setup counters\n\t\tthis.docFrequencyCount[categoryName] = 0;\n\t\tthis.wordFrequencyCount[categoryName] = {};\n\t\tthis.wordCount[categoryName] = 0;\n\n\t\t//add new category to our list\n\t\tthis.categories[categoryName] = true;\n\t}\n\treturn this.categories[categoryName] ? categoryName : undefined;\n};\n\n/**\n * Build a frequency hashmap where\n * - the keys are the entries in `tokens`\n * - the values are the frequency of each entry (`tokens`)\n *\n * @param  {Array} tokens - Normalized word array\n * @return {Object} FrequencyTable\n */\nNaiveBayesClassifier.prototype.FrequencyTable = function(tokens) {\n\tvar frequencyTable = {};\n\n\ttokens.forEach(function (token) {\n\t\tif (!frequencyTable[token]) {\n\t\t\tfrequencyTable[token] = 1;\n\t\t} else {\n\t\t\tfrequencyTable[token] += 1;\n\t\t}\n\t});\n\n\treturn frequencyTable;\n};\n\n/**\n * Train our naive-bayes classifier by telling it what `category` some `text` corresponds to.\n *\n * @param  {String} text\n * @param  {String} category\n * @return {Object} NaiveBayesClassifier\n */\nNaiveBayesClassifier.prototype.learn = function(text, category) {\n\tvar self = this; //get reference to instance\n\n\tcategory = self.Category(category); //get or create a category\n\n\tself.docFrequencyCount[category] += 1; //update our count of how many documents mapped to this category\n\tself.totalNumberOfDocuments += 1; //update the total number of documents we have learned from\n\n\tvar tokens = self.tokenizer(text); //break up the text into tokens\n\tvar tokenFrequencyTable = self.FrequencyTable(tokens); //get a frequency count for each token in the text\n\n\t// Update our vocabulary and our word frequency counts for this category\n\t// =============================================================================\n\tObject\n\t.keys(tokenFrequencyTable)\n\t.forEach(function(token) { //for each token in our tokenFrequencyTable\n\t\t\n\t\tself.addWordToVocabulary(token); //add this word to our vocabulary if not already existing\n\n\t\tvar frequencyOfTokenInText = tokenFrequencyTable[token]; //look it up once, for speed\n\n\t\t//update the frequency information for this word in this category\n\t\tif (!self.wordFrequencyCount[category][token]) {\n\t\t\tself.wordFrequencyCount[category][token] = frequencyOfTokenInText; //set it for the first time\n\t\t} else {\n\t\t\tself.wordFrequencyCount[category][token] += frequencyOfTokenInText; //add to what's already there in the count\n\t\t}\n\n\t\tself.wordCount[category] += frequencyOfTokenInText; //add to the count of all words we have seen mapped to this category\n\t});\n\n\treturn self;\n};\n\n/**\n * Calculate probability that a `token` belongs to a `category`\n *\n * @param  {String} token\n * @param  {String} category\n * @return {Number} probability\n */\nNaiveBayesClassifier.prototype.tokenProbability = function(token, category) {\n\t// Recall => P(wi|Cj) = count(wi,cj) / SUM[(for w in v) count(w,cj)]\n\t// =============================================================================\n\n\t//how many times this word has occurred in documents mapped to this category\n\tvar wordFrequencyCount = this.wordFrequencyCount[category][token] || 0; //count(wi,cj)\n\n\t//what is the count of all words that have ever been mapped to this category\n\tvar wordCount = this.wordCount[category]; //SUM[(for w in v) count(w,cj)\n\n\t//use laplace Add-1 Smoothing equation\n\t//=> ( P(wi|Cj) = count(wi,cj) + 1 ) / ( SUM[(for w in v) count(w,cj)] + |VocabSize| )\n\treturn ( wordFrequencyCount + 1 ) / ( wordCount + this.getVocabularySize() );\n};\n\n/**\n * Determine what category `text` belongs to.\n * Use Laplace (add-1) smoothing to adjust for words that do not appear in our vocabulary (unknown words)\n *\n * @param  {String} text\n * @return {Object} cMAP and categories\n */\nNaiveBayesClassifier.prototype.categorize = function (text) {\n\tvar self = this,  //get reference to instance\n\t\t\tmaxProbability = -Infinity,\n\t\t\ttotalProbabilities = 0,\n\t\t\tcMAP = {}, //category of “maximum a posteriori” => most likely category\n\t\t\tcategoryProbabilities = {}; //probabilities of all categories\n\n\tvar tokens = self.tokenizer(text),\n\t\ttokenFrequencyTable = self.FrequencyTable(tokens);\n\n\tObject\n\t.keys(self.categories)\n\t.forEach(function(category) { //for each category, find the probability of the text belonging to it\n\t\tif (!self.categories[category]) { return; } //ignore categories that have been switched off\n\n\t\t// 1. Find overall probability of this category\n\t\t//=> P(Cj) = docCount(C=cj) / Ndoc\n\t\t// =============================================================================\n\t\t\n\t\t//Put of all documents we've ever looked at, how many were mapped to this category\n\t\tvar categoryProbability = self.docFrequencyCount[category] / self.totalNumberOfDocuments;\n\n\t\t//take the log to avoid underflow with large datasets - http://www.johndcook.com/blog/2012/07/26/avoiding-underflow-in-bayesian-computations/\n\t\tvar logCategoryProbability = Math.log(categoryProbability); //start with P(Cj), we will add P(wi|Cj) incrementally below\n\n\t\t// 2. Find probability of each word in this categogy\n\t\t//=> P(wi|Cj) = count(wi,cj) / SUM[(for w in v) count(w,cj)]\n\t\t// =============================================================================\n\n\t\tObject\n\t\t.keys(tokenFrequencyTable)\n\t\t.forEach(function(token) { //for each token in our token frequency table\n\n\t\t\t//determine the log of the probability of this token belonging to the current category\n\t\t\t//=> log( P(w|c) )\n\t\t\tvar tokenProbability = self.tokenProbability(token, category);\n\t\t\t//and add it to our running probability that the text belongs to the current category\n\t\t\tlogCategoryProbability += Math.log(tokenProbability); //TODO: look into *frequencyTable[token];\n\n\t\t\t// console.log('token: %s | category: `%s` | probability: %d', token, category, tokenProbability);\n\t\t});\n\n\t\t// 3. Find the most likely category, thus far...\n\t\t// =============================================================================\n\n\t\tcategoryProbability = Math.exp(logCategoryProbability); //reverse the log and get an actual value\n\t\ttotalProbabilities += categoryProbability; //calculate totals as we go, we'll use this to normalise later\n\n\t\tif (logCategoryProbability > maxProbability) { //find cMAP\n\t\t\tmaxProbability = logCategoryProbability;\n\t\t\tcMAP = {\n\t\t\t\tcategory: category,\n\t\t\t\tprobability: categoryProbability\n\t\t\t};\n\t\t}\n\n\t\tcategoryProbabilities[category] = categoryProbability;\n\t});\n\n\t//normalise (out of 1) the probabilities, so that they make a bit more sense to the average person\n\tObject\n\t.keys(categoryProbabilities)\n\t.forEach(function(category) {\n\t\tcategoryProbabilities[category] /= totalProbabilities;\n\t});\n\n\treturn {\n\t\tcategory: cMAP.category || 'unclassified',\n\t\tprobability: (cMAP.probability /= totalProbabilities) || -Infinity,\n\t\tcategories: categoryProbabilities\n\t};\n};"},"name":"VERSION","memberof":"NaiveBayesClassifier","scope":"static"}
,
{"description":"Determine what category `text` belongs to.\nUse Laplace (add-1) smoothing to adjust for words that do not appear in our vocabulary (unknown words)","tags":[{"title":"param","description":null,"type":{"type":"NameExpression","name":"String"},"name":"text"},{"title":"returns","description":"cMAP and categories","type":{"type":"NameExpression","name":"Object"}},{"title":"name","name":"categorize"},{"title":"kind","kind":"function"},{"title":"memberof","description":"NaiveBayesClassifier"},{"title":"instance"}],"context":{"loc":{"start":{"line":225,"column":0},"end":{"line":296,"column":2}},"file":"/Users/hsalem/Developer/naive-bayes-classifier/src/NaiveBayesClassifier.js","code":"'use strict';\n\n/**\n * NaiveBayesClassifier constructor fuction. Takes an (optional) options object containing:\n *   - {Function} tokenizer => custom tokenization function\n *\n * @constructor\n * @param  {Object} options - Options to be used for intialisation\n * @return {Object} NaiveBayesClassifier\n */\nvar NaiveBayesClassifier = function(options) {\n\t// OPTIONS\n\t// =============================================================================\n\tthis.options = {};\n\n\tif (!options) {\n\t\tif (typeof options !== 'object') {\n\t\t\tthrow new TypeError('NaiveBayesClassifier got invalid `options`: `' + options + '`. Please pass in an object.');\n\t\t}\n\t\tthis.options = options;\n\t}\n\n\t// TOKENIZER\n\t// =============================================================================\n\t/**\n\t * Given an input string, tokenize it into an array of word tokens.\n\t * This tokenizer adopts a naive \"Bag of words\" assumption.\n\t * This is the default tokenization function used if the user does not provide one in `options`.\n\t *\n\t * @private\n\t * @param  {String} text\n\t * @return {Array}\n\t */\n\tvar defaultTokenizer = function(text) {\n\t\t//remove punctuation from text (anything that isn't a word char or a space), and enforce lowercase\n\t\tvar rgxPunctuation = new RegExp(/[^\\w\\s]/g);\n\t\tvar sanitized = text.replace(rgxPunctuation, ' ').toLowerCase();\n\n\t\treturn sanitized.split(/\\s+/);\n\t};\n\n\tthis.tokenizer = this.options.tokenizer || defaultTokenizer;\n\t\n\t// VOCABULARY\n\t// =============================================================================\n\t//initialize our vocabulary and its size\n\tthis.vocabulary = {};\n\tthis.vocabularySize = 0;\n\n\t// CATEGORIES - hashmap of our category names\n\t// =============================================================================\n\tthis.categories = {};\n\n\t// PRIOR PROBABILITY: P(Cj) = docCount(C=cj) / Ndoc\n\t// =============================================================================\n\n\t//document frequency table for each of our categories\n\t//=> for each category, how often were documents mapped to it\n\tthis.docFrequencyCount = {}; //docCount(class)\n\n\tthis.totalNumberOfDocuments = 0; //Ndoc => number of documents we have learned from\n\n\t// LIKELIHOOD: P(wi|Cj) = count(wi,cj) / SUM[(for w in v) count(w,cj)]\n\t// =============================================================================\n\n\t//word frequency table for each of our categories\n\t//=> for each category, how frequent was a given word mapped to it\n\tthis.wordFrequencyCount = {}; //count(wi,cj)\n\n\t//word count table for each of our categories\n\t//=> for each category, how many words in total were mapped to it\n\tthis.wordCount = {}; //SUM[(for w in v) count(w,cj)\n};\n\n/**\n * Library version number\n */\nNaiveBayesClassifier.VERSION = '0.1.0'; // current version | Note: JS Functions are first class Objects\n\nNaiveBayesClassifier.withClassifier = function(classifier) {\n\treturn new NaiveBayesClassifier(classifier.options);\n};\n\nNaiveBayesClassifier.prototype.addWordToVocabulary = function(word) {\n\tif (!this.vocabulary[word]) {\n\t\tthis.vocabulary[word] = true;\n\t\tthis.vocabularySize += 1;\n\t}\n};\nNaiveBayesClassifier.prototype.getVocabularySize = function() {\n\tthis.vocabularySize = Object.keys(this.vocabulary).length;\n\treturn this.vocabularySize;\n};\n\n/**\n * Initialize each of our data structure entries for this new category\n *\n * @param  {String} categoryName - Name of the category you want to get or create\n * @return {String} category\n */\nNaiveBayesClassifier.prototype.getOrCreateCategory = function(categoryName) {\n\tif (!categoryName || typeof categoryName !== 'string') {\n\t\tthrow new TypeError('Category creator got invalid category name: `' + categoryName + '`. Please pass in a String.');\n\t}\n\n\t//simple singleton for each category\n\tif (!this.categories[categoryName]) {\n\t\t//setup counters\n\t\tthis.docFrequencyCount[categoryName] = 0;\n\t\tthis.wordFrequencyCount[categoryName] = {};\n\t\tthis.wordCount[categoryName] = 0;\n\n\t\t//add new category to our list\n\t\tthis.categories[categoryName] = true;\n\t}\n\treturn this.categories[categoryName] ? categoryName : undefined;\n};\n\n/**\n * Build a frequency hashmap where\n * - the keys are the entries in `tokens`\n * - the values are the frequency of each entry (`tokens`)\n *\n * @param  {Array} tokens - Normalized word array\n * @return {Object} FrequencyTable\n */\nNaiveBayesClassifier.prototype.FrequencyTable = function(tokens) {\n\tvar frequencyTable = {};\n\n\ttokens.forEach(function (token) {\n\t\tif (!frequencyTable[token]) {\n\t\t\tfrequencyTable[token] = 1;\n\t\t} else {\n\t\t\tfrequencyTable[token] += 1;\n\t\t}\n\t});\n\n\treturn frequencyTable;\n};\n\n/**\n * Train our naive-bayes classifier by telling it what `category` some `text` corresponds to.\n *\n * @param  {String} text\n * @param  {String} category\n * @return {Object} NaiveBayesClassifier\n */\nNaiveBayesClassifier.prototype.learn = function(text, category) {\n\tvar self = this; //get reference to instance\n\n\tcategory = self.Category(category); //get or create a category\n\n\tself.docFrequencyCount[category] += 1; //update our count of how many documents mapped to this category\n\tself.totalNumberOfDocuments += 1; //update the total number of documents we have learned from\n\n\tvar tokens = self.tokenizer(text); //break up the text into tokens\n\tvar tokenFrequencyTable = self.FrequencyTable(tokens); //get a frequency count for each token in the text\n\n\t// Update our vocabulary and our word frequency counts for this category\n\t// =============================================================================\n\tObject\n\t.keys(tokenFrequencyTable)\n\t.forEach(function(token) { //for each token in our tokenFrequencyTable\n\t\t\n\t\tself.addWordToVocabulary(token); //add this word to our vocabulary if not already existing\n\n\t\tvar frequencyOfTokenInText = tokenFrequencyTable[token]; //look it up once, for speed\n\n\t\t//update the frequency information for this word in this category\n\t\tif (!self.wordFrequencyCount[category][token]) {\n\t\t\tself.wordFrequencyCount[category][token] = frequencyOfTokenInText; //set it for the first time\n\t\t} else {\n\t\t\tself.wordFrequencyCount[category][token] += frequencyOfTokenInText; //add to what's already there in the count\n\t\t}\n\n\t\tself.wordCount[category] += frequencyOfTokenInText; //add to the count of all words we have seen mapped to this category\n\t});\n\n\treturn self;\n};\n\n/**\n * Calculate probability that a `token` belongs to a `category`\n *\n * @param  {String} token\n * @param  {String} category\n * @return {Number} probability\n */\nNaiveBayesClassifier.prototype.tokenProbability = function(token, category) {\n\t// Recall => P(wi|Cj) = count(wi,cj) / SUM[(for w in v) count(w,cj)]\n\t// =============================================================================\n\n\t//how many times this word has occurred in documents mapped to this category\n\tvar wordFrequencyCount = this.wordFrequencyCount[category][token] || 0; //count(wi,cj)\n\n\t//what is the count of all words that have ever been mapped to this category\n\tvar wordCount = this.wordCount[category]; //SUM[(for w in v) count(w,cj)\n\n\t//use laplace Add-1 Smoothing equation\n\t//=> ( P(wi|Cj) = count(wi,cj) + 1 ) / ( SUM[(for w in v) count(w,cj)] + |VocabSize| )\n\treturn ( wordFrequencyCount + 1 ) / ( wordCount + this.getVocabularySize() );\n};\n\n/**\n * Determine what category `text` belongs to.\n * Use Laplace (add-1) smoothing to adjust for words that do not appear in our vocabulary (unknown words)\n *\n * @param  {String} text\n * @return {Object} cMAP and categories\n */\nNaiveBayesClassifier.prototype.categorize = function (text) {\n\tvar self = this,  //get reference to instance\n\t\t\tmaxProbability = -Infinity,\n\t\t\ttotalProbabilities = 0,\n\t\t\tcMAP = {}, //category of “maximum a posteriori” => most likely category\n\t\t\tcategoryProbabilities = {}; //probabilities of all categories\n\n\tvar tokens = self.tokenizer(text),\n\t\ttokenFrequencyTable = self.FrequencyTable(tokens);\n\n\tObject\n\t.keys(self.categories)\n\t.forEach(function(category) { //for each category, find the probability of the text belonging to it\n\t\tif (!self.categories[category]) { return; } //ignore categories that have been switched off\n\n\t\t// 1. Find overall probability of this category\n\t\t//=> P(Cj) = docCount(C=cj) / Ndoc\n\t\t// =============================================================================\n\t\t\n\t\t//Put of all documents we've ever looked at, how many were mapped to this category\n\t\tvar categoryProbability = self.docFrequencyCount[category] / self.totalNumberOfDocuments;\n\n\t\t//take the log to avoid underflow with large datasets - http://www.johndcook.com/blog/2012/07/26/avoiding-underflow-in-bayesian-computations/\n\t\tvar logCategoryProbability = Math.log(categoryProbability); //start with P(Cj), we will add P(wi|Cj) incrementally below\n\n\t\t// 2. Find probability of each word in this categogy\n\t\t//=> P(wi|Cj) = count(wi,cj) / SUM[(for w in v) count(w,cj)]\n\t\t// =============================================================================\n\n\t\tObject\n\t\t.keys(tokenFrequencyTable)\n\t\t.forEach(function(token) { //for each token in our token frequency table\n\n\t\t\t//determine the log of the probability of this token belonging to the current category\n\t\t\t//=> log( P(w|c) )\n\t\t\tvar tokenProbability = self.tokenProbability(token, category);\n\t\t\t//and add it to our running probability that the text belongs to the current category\n\t\t\tlogCategoryProbability += Math.log(tokenProbability); //TODO: look into *frequencyTable[token];\n\n\t\t\t// console.log('token: %s | category: `%s` | probability: %d', token, category, tokenProbability);\n\t\t});\n\n\t\t// 3. Find the most likely category, thus far...\n\t\t// =============================================================================\n\n\t\tcategoryProbability = Math.exp(logCategoryProbability); //reverse the log and get an actual value\n\t\ttotalProbabilities += categoryProbability; //calculate totals as we go, we'll use this to normalise later\n\n\t\tif (logCategoryProbability > maxProbability) { //find cMAP\n\t\t\tmaxProbability = logCategoryProbability;\n\t\t\tcMAP = {\n\t\t\t\tcategory: category,\n\t\t\t\tprobability: categoryProbability\n\t\t\t};\n\t\t}\n\n\t\tcategoryProbabilities[category] = categoryProbability;\n\t});\n\n\t//normalise (out of 1) the probabilities, so that they make a bit more sense to the average person\n\tObject\n\t.keys(categoryProbabilities)\n\t.forEach(function(category) {\n\t\tcategoryProbabilities[category] /= totalProbabilities;\n\t});\n\n\treturn {\n\t\tcategory: cMAP.category || 'unclassified',\n\t\tprobability: (cMAP.probability /= totalProbabilities) || -Infinity,\n\t\tcategories: categoryProbabilities\n\t};\n};"},"params":[{"title":"param","description":null,"type":{"type":"NameExpression","name":"String"},"name":"text"}],"returns":[{"title":"returns","description":"cMAP and categories","type":{"type":"NameExpression","name":"Object"}}],"name":"categorize","kind":"function","memberof":"NaiveBayesClassifier","scope":"instance"}
,
{"description":"Initialize each of our data structure entries for this new category","tags":[{"title":"param","description":"Name of the category you want to get or create","type":{"type":"NameExpression","name":"String"},"name":"categoryName"},{"title":"returns","description":"category","type":{"type":"NameExpression","name":"String"}},{"title":"name","name":"getOrCreateCategory"},{"title":"kind","kind":"function"},{"title":"memberof","description":"NaiveBayesClassifier"},{"title":"instance"}],"context":{"loc":{"start":{"line":115,"column":0},"end":{"line":131,"column":2}},"file":"/Users/hsalem/Developer/naive-bayes-classifier/src/NaiveBayesClassifier.js","code":"'use strict';\n\n/**\n * NaiveBayesClassifier constructor fuction. Takes an (optional) options object containing:\n *   - {Function} tokenizer => custom tokenization function\n *\n * @constructor\n * @param  {Object} options - Options to be used for intialisation\n * @return {Object} NaiveBayesClassifier\n */\nvar NaiveBayesClassifier = function(options) {\n\t// OPTIONS\n\t// =============================================================================\n\tthis.options = {};\n\n\tif (!options) {\n\t\tif (typeof options !== 'object') {\n\t\t\tthrow new TypeError('NaiveBayesClassifier got invalid `options`: `' + options + '`. Please pass in an object.');\n\t\t}\n\t\tthis.options = options;\n\t}\n\n\t// TOKENIZER\n\t// =============================================================================\n\t/**\n\t * Given an input string, tokenize it into an array of word tokens.\n\t * This tokenizer adopts a naive \"Bag of words\" assumption.\n\t * This is the default tokenization function used if the user does not provide one in `options`.\n\t *\n\t * @private\n\t * @param  {String} text\n\t * @return {Array}\n\t */\n\tvar defaultTokenizer = function(text) {\n\t\t//remove punctuation from text (anything that isn't a word char or a space), and enforce lowercase\n\t\tvar rgxPunctuation = new RegExp(/[^\\w\\s]/g);\n\t\tvar sanitized = text.replace(rgxPunctuation, ' ').toLowerCase();\n\n\t\treturn sanitized.split(/\\s+/);\n\t};\n\n\tthis.tokenizer = this.options.tokenizer || defaultTokenizer;\n\t\n\t// VOCABULARY\n\t// =============================================================================\n\t//initialize our vocabulary and its size\n\tthis.vocabulary = {};\n\tthis.vocabularySize = 0;\n\n\t// CATEGORIES - hashmap of our category names\n\t// =============================================================================\n\tthis.categories = {};\n\n\t// PRIOR PROBABILITY: P(Cj) = docCount(C=cj) / Ndoc\n\t// =============================================================================\n\n\t//document frequency table for each of our categories\n\t//=> for each category, how often were documents mapped to it\n\tthis.docFrequencyCount = {}; //docCount(class)\n\n\tthis.totalNumberOfDocuments = 0; //Ndoc => number of documents we have learned from\n\n\t// LIKELIHOOD: P(wi|Cj) = count(wi,cj) / SUM[(for w in v) count(w,cj)]\n\t// =============================================================================\n\n\t//word frequency table for each of our categories\n\t//=> for each category, how frequent was a given word mapped to it\n\tthis.wordFrequencyCount = {}; //count(wi,cj)\n\n\t//word count table for each of our categories\n\t//=> for each category, how many words in total were mapped to it\n\tthis.wordCount = {}; //SUM[(for w in v) count(w,cj)\n};\n\n/**\n * Library version number\n */\nNaiveBayesClassifier.VERSION = '0.1.0'; // current version | Note: JS Functions are first class Objects\n\nNaiveBayesClassifier.withClassifier = function(classifier) {\n\treturn new NaiveBayesClassifier(classifier.options);\n};\n\nNaiveBayesClassifier.prototype.addWordToVocabulary = function(word) {\n\tif (!this.vocabulary[word]) {\n\t\tthis.vocabulary[word] = true;\n\t\tthis.vocabularySize += 1;\n\t}\n};\nNaiveBayesClassifier.prototype.getVocabularySize = function() {\n\tthis.vocabularySize = Object.keys(this.vocabulary).length;\n\treturn this.vocabularySize;\n};\n\n/**\n * Initialize each of our data structure entries for this new category\n *\n * @param  {String} categoryName - Name of the category you want to get or create\n * @return {String} category\n */\nNaiveBayesClassifier.prototype.getOrCreateCategory = function(categoryName) {\n\tif (!categoryName || typeof categoryName !== 'string') {\n\t\tthrow new TypeError('Category creator got invalid category name: `' + categoryName + '`. Please pass in a String.');\n\t}\n\n\t//simple singleton for each category\n\tif (!this.categories[categoryName]) {\n\t\t//setup counters\n\t\tthis.docFrequencyCount[categoryName] = 0;\n\t\tthis.wordFrequencyCount[categoryName] = {};\n\t\tthis.wordCount[categoryName] = 0;\n\n\t\t//add new category to our list\n\t\tthis.categories[categoryName] = true;\n\t}\n\treturn this.categories[categoryName] ? categoryName : undefined;\n};\n\n/**\n * Build a frequency hashmap where\n * - the keys are the entries in `tokens`\n * - the values are the frequency of each entry (`tokens`)\n *\n * @param  {Array} tokens - Normalized word array\n * @return {Object} FrequencyTable\n */\nNaiveBayesClassifier.prototype.FrequencyTable = function(tokens) {\n\tvar frequencyTable = {};\n\n\ttokens.forEach(function (token) {\n\t\tif (!frequencyTable[token]) {\n\t\t\tfrequencyTable[token] = 1;\n\t\t} else {\n\t\t\tfrequencyTable[token] += 1;\n\t\t}\n\t});\n\n\treturn frequencyTable;\n};\n\n/**\n * Train our naive-bayes classifier by telling it what `category` some `text` corresponds to.\n *\n * @param  {String} text\n * @param  {String} category\n * @return {Object} NaiveBayesClassifier\n */\nNaiveBayesClassifier.prototype.learn = function(text, category) {\n\tvar self = this; //get reference to instance\n\n\tcategory = self.Category(category); //get or create a category\n\n\tself.docFrequencyCount[category] += 1; //update our count of how many documents mapped to this category\n\tself.totalNumberOfDocuments += 1; //update the total number of documents we have learned from\n\n\tvar tokens = self.tokenizer(text); //break up the text into tokens\n\tvar tokenFrequencyTable = self.FrequencyTable(tokens); //get a frequency count for each token in the text\n\n\t// Update our vocabulary and our word frequency counts for this category\n\t// =============================================================================\n\tObject\n\t.keys(tokenFrequencyTable)\n\t.forEach(function(token) { //for each token in our tokenFrequencyTable\n\t\t\n\t\tself.addWordToVocabulary(token); //add this word to our vocabulary if not already existing\n\n\t\tvar frequencyOfTokenInText = tokenFrequencyTable[token]; //look it up once, for speed\n\n\t\t//update the frequency information for this word in this category\n\t\tif (!self.wordFrequencyCount[category][token]) {\n\t\t\tself.wordFrequencyCount[category][token] = frequencyOfTokenInText; //set it for the first time\n\t\t} else {\n\t\t\tself.wordFrequencyCount[category][token] += frequencyOfTokenInText; //add to what's already there in the count\n\t\t}\n\n\t\tself.wordCount[category] += frequencyOfTokenInText; //add to the count of all words we have seen mapped to this category\n\t});\n\n\treturn self;\n};\n\n/**\n * Calculate probability that a `token` belongs to a `category`\n *\n * @param  {String} token\n * @param  {String} category\n * @return {Number} probability\n */\nNaiveBayesClassifier.prototype.tokenProbability = function(token, category) {\n\t// Recall => P(wi|Cj) = count(wi,cj) / SUM[(for w in v) count(w,cj)]\n\t// =============================================================================\n\n\t//how many times this word has occurred in documents mapped to this category\n\tvar wordFrequencyCount = this.wordFrequencyCount[category][token] || 0; //count(wi,cj)\n\n\t//what is the count of all words that have ever been mapped to this category\n\tvar wordCount = this.wordCount[category]; //SUM[(for w in v) count(w,cj)\n\n\t//use laplace Add-1 Smoothing equation\n\t//=> ( P(wi|Cj) = count(wi,cj) + 1 ) / ( SUM[(for w in v) count(w,cj)] + |VocabSize| )\n\treturn ( wordFrequencyCount + 1 ) / ( wordCount + this.getVocabularySize() );\n};\n\n/**\n * Determine what category `text` belongs to.\n * Use Laplace (add-1) smoothing to adjust for words that do not appear in our vocabulary (unknown words)\n *\n * @param  {String} text\n * @return {Object} cMAP and categories\n */\nNaiveBayesClassifier.prototype.categorize = function (text) {\n\tvar self = this,  //get reference to instance\n\t\t\tmaxProbability = -Infinity,\n\t\t\ttotalProbabilities = 0,\n\t\t\tcMAP = {}, //category of “maximum a posteriori” => most likely category\n\t\t\tcategoryProbabilities = {}; //probabilities of all categories\n\n\tvar tokens = self.tokenizer(text),\n\t\ttokenFrequencyTable = self.FrequencyTable(tokens);\n\n\tObject\n\t.keys(self.categories)\n\t.forEach(function(category) { //for each category, find the probability of the text belonging to it\n\t\tif (!self.categories[category]) { return; } //ignore categories that have been switched off\n\n\t\t// 1. Find overall probability of this category\n\t\t//=> P(Cj) = docCount(C=cj) / Ndoc\n\t\t// =============================================================================\n\t\t\n\t\t//Put of all documents we've ever looked at, how many were mapped to this category\n\t\tvar categoryProbability = self.docFrequencyCount[category] / self.totalNumberOfDocuments;\n\n\t\t//take the log to avoid underflow with large datasets - http://www.johndcook.com/blog/2012/07/26/avoiding-underflow-in-bayesian-computations/\n\t\tvar logCategoryProbability = Math.log(categoryProbability); //start with P(Cj), we will add P(wi|Cj) incrementally below\n\n\t\t// 2. Find probability of each word in this categogy\n\t\t//=> P(wi|Cj) = count(wi,cj) / SUM[(for w in v) count(w,cj)]\n\t\t// =============================================================================\n\n\t\tObject\n\t\t.keys(tokenFrequencyTable)\n\t\t.forEach(function(token) { //for each token in our token frequency table\n\n\t\t\t//determine the log of the probability of this token belonging to the current category\n\t\t\t//=> log( P(w|c) )\n\t\t\tvar tokenProbability = self.tokenProbability(token, category);\n\t\t\t//and add it to our running probability that the text belongs to the current category\n\t\t\tlogCategoryProbability += Math.log(tokenProbability); //TODO: look into *frequencyTable[token];\n\n\t\t\t// console.log('token: %s | category: `%s` | probability: %d', token, category, tokenProbability);\n\t\t});\n\n\t\t// 3. Find the most likely category, thus far...\n\t\t// =============================================================================\n\n\t\tcategoryProbability = Math.exp(logCategoryProbability); //reverse the log and get an actual value\n\t\ttotalProbabilities += categoryProbability; //calculate totals as we go, we'll use this to normalise later\n\n\t\tif (logCategoryProbability > maxProbability) { //find cMAP\n\t\t\tmaxProbability = logCategoryProbability;\n\t\t\tcMAP = {\n\t\t\t\tcategory: category,\n\t\t\t\tprobability: categoryProbability\n\t\t\t};\n\t\t}\n\n\t\tcategoryProbabilities[category] = categoryProbability;\n\t});\n\n\t//normalise (out of 1) the probabilities, so that they make a bit more sense to the average person\n\tObject\n\t.keys(categoryProbabilities)\n\t.forEach(function(category) {\n\t\tcategoryProbabilities[category] /= totalProbabilities;\n\t});\n\n\treturn {\n\t\tcategory: cMAP.category || 'unclassified',\n\t\tprobability: (cMAP.probability /= totalProbabilities) || -Infinity,\n\t\tcategories: categoryProbabilities\n\t};\n};"},"params":[{"title":"param","description":"Name of the category you want to get or create","type":{"type":"NameExpression","name":"String"},"name":"categoryName"}],"returns":[{"title":"returns","description":"category","type":{"type":"NameExpression","name":"String"}}],"name":"getOrCreateCategory","kind":"function","memberof":"NaiveBayesClassifier","scope":"instance"}
,
{"description":"Train our naive-bayes classifier by telling it what `category` some `text` corresponds to.","tags":[{"title":"param","description":null,"type":{"type":"NameExpression","name":"String"},"name":"text"},{"title":"param","description":null,"type":{"type":"NameExpression","name":"String"},"name":"category"},{"title":"returns","description":"NaiveBayesClassifier","type":{"type":"NameExpression","name":"Object"}},{"title":"name","name":"learn"},{"title":"kind","kind":"function"},{"title":"memberof","description":"NaiveBayesClassifier"},{"title":"instance"}],"context":{"loc":{"start":{"line":162,"column":0},"end":{"line":194,"column":2}},"file":"/Users/hsalem/Developer/naive-bayes-classifier/src/NaiveBayesClassifier.js","code":"'use strict';\n\n/**\n * NaiveBayesClassifier constructor fuction. Takes an (optional) options object containing:\n *   - {Function} tokenizer => custom tokenization function\n *\n * @constructor\n * @param  {Object} options - Options to be used for intialisation\n * @return {Object} NaiveBayesClassifier\n */\nvar NaiveBayesClassifier = function(options) {\n\t// OPTIONS\n\t// =============================================================================\n\tthis.options = {};\n\n\tif (!options) {\n\t\tif (typeof options !== 'object') {\n\t\t\tthrow new TypeError('NaiveBayesClassifier got invalid `options`: `' + options + '`. Please pass in an object.');\n\t\t}\n\t\tthis.options = options;\n\t}\n\n\t// TOKENIZER\n\t// =============================================================================\n\t/**\n\t * Given an input string, tokenize it into an array of word tokens.\n\t * This tokenizer adopts a naive \"Bag of words\" assumption.\n\t * This is the default tokenization function used if the user does not provide one in `options`.\n\t *\n\t * @private\n\t * @param  {String} text\n\t * @return {Array}\n\t */\n\tvar defaultTokenizer = function(text) {\n\t\t//remove punctuation from text (anything that isn't a word char or a space), and enforce lowercase\n\t\tvar rgxPunctuation = new RegExp(/[^\\w\\s]/g);\n\t\tvar sanitized = text.replace(rgxPunctuation, ' ').toLowerCase();\n\n\t\treturn sanitized.split(/\\s+/);\n\t};\n\n\tthis.tokenizer = this.options.tokenizer || defaultTokenizer;\n\t\n\t// VOCABULARY\n\t// =============================================================================\n\t//initialize our vocabulary and its size\n\tthis.vocabulary = {};\n\tthis.vocabularySize = 0;\n\n\t// CATEGORIES - hashmap of our category names\n\t// =============================================================================\n\tthis.categories = {};\n\n\t// PRIOR PROBABILITY: P(Cj) = docCount(C=cj) / Ndoc\n\t// =============================================================================\n\n\t//document frequency table for each of our categories\n\t//=> for each category, how often were documents mapped to it\n\tthis.docFrequencyCount = {}; //docCount(class)\n\n\tthis.totalNumberOfDocuments = 0; //Ndoc => number of documents we have learned from\n\n\t// LIKELIHOOD: P(wi|Cj) = count(wi,cj) / SUM[(for w in v) count(w,cj)]\n\t// =============================================================================\n\n\t//word frequency table for each of our categories\n\t//=> for each category, how frequent was a given word mapped to it\n\tthis.wordFrequencyCount = {}; //count(wi,cj)\n\n\t//word count table for each of our categories\n\t//=> for each category, how many words in total were mapped to it\n\tthis.wordCount = {}; //SUM[(for w in v) count(w,cj)\n};\n\n/**\n * Library version number\n */\nNaiveBayesClassifier.VERSION = '0.1.0'; // current version | Note: JS Functions are first class Objects\n\nNaiveBayesClassifier.withClassifier = function(classifier) {\n\treturn new NaiveBayesClassifier(classifier.options);\n};\n\nNaiveBayesClassifier.prototype.addWordToVocabulary = function(word) {\n\tif (!this.vocabulary[word]) {\n\t\tthis.vocabulary[word] = true;\n\t\tthis.vocabularySize += 1;\n\t}\n};\nNaiveBayesClassifier.prototype.getVocabularySize = function() {\n\tthis.vocabularySize = Object.keys(this.vocabulary).length;\n\treturn this.vocabularySize;\n};\n\n/**\n * Initialize each of our data structure entries for this new category\n *\n * @param  {String} categoryName - Name of the category you want to get or create\n * @return {String} category\n */\nNaiveBayesClassifier.prototype.getOrCreateCategory = function(categoryName) {\n\tif (!categoryName || typeof categoryName !== 'string') {\n\t\tthrow new TypeError('Category creator got invalid category name: `' + categoryName + '`. Please pass in a String.');\n\t}\n\n\t//simple singleton for each category\n\tif (!this.categories[categoryName]) {\n\t\t//setup counters\n\t\tthis.docFrequencyCount[categoryName] = 0;\n\t\tthis.wordFrequencyCount[categoryName] = {};\n\t\tthis.wordCount[categoryName] = 0;\n\n\t\t//add new category to our list\n\t\tthis.categories[categoryName] = true;\n\t}\n\treturn this.categories[categoryName] ? categoryName : undefined;\n};\n\n/**\n * Build a frequency hashmap where\n * - the keys are the entries in `tokens`\n * - the values are the frequency of each entry (`tokens`)\n *\n * @param  {Array} tokens - Normalized word array\n * @return {Object} FrequencyTable\n */\nNaiveBayesClassifier.prototype.FrequencyTable = function(tokens) {\n\tvar frequencyTable = {};\n\n\ttokens.forEach(function (token) {\n\t\tif (!frequencyTable[token]) {\n\t\t\tfrequencyTable[token] = 1;\n\t\t} else {\n\t\t\tfrequencyTable[token] += 1;\n\t\t}\n\t});\n\n\treturn frequencyTable;\n};\n\n/**\n * Train our naive-bayes classifier by telling it what `category` some `text` corresponds to.\n *\n * @param  {String} text\n * @param  {String} category\n * @return {Object} NaiveBayesClassifier\n */\nNaiveBayesClassifier.prototype.learn = function(text, category) {\n\tvar self = this; //get reference to instance\n\n\tcategory = self.Category(category); //get or create a category\n\n\tself.docFrequencyCount[category] += 1; //update our count of how many documents mapped to this category\n\tself.totalNumberOfDocuments += 1; //update the total number of documents we have learned from\n\n\tvar tokens = self.tokenizer(text); //break up the text into tokens\n\tvar tokenFrequencyTable = self.FrequencyTable(tokens); //get a frequency count for each token in the text\n\n\t// Update our vocabulary and our word frequency counts for this category\n\t// =============================================================================\n\tObject\n\t.keys(tokenFrequencyTable)\n\t.forEach(function(token) { //for each token in our tokenFrequencyTable\n\t\t\n\t\tself.addWordToVocabulary(token); //add this word to our vocabulary if not already existing\n\n\t\tvar frequencyOfTokenInText = tokenFrequencyTable[token]; //look it up once, for speed\n\n\t\t//update the frequency information for this word in this category\n\t\tif (!self.wordFrequencyCount[category][token]) {\n\t\t\tself.wordFrequencyCount[category][token] = frequencyOfTokenInText; //set it for the first time\n\t\t} else {\n\t\t\tself.wordFrequencyCount[category][token] += frequencyOfTokenInText; //add to what's already there in the count\n\t\t}\n\n\t\tself.wordCount[category] += frequencyOfTokenInText; //add to the count of all words we have seen mapped to this category\n\t});\n\n\treturn self;\n};\n\n/**\n * Calculate probability that a `token` belongs to a `category`\n *\n * @param  {String} token\n * @param  {String} category\n * @return {Number} probability\n */\nNaiveBayesClassifier.prototype.tokenProbability = function(token, category) {\n\t// Recall => P(wi|Cj) = count(wi,cj) / SUM[(for w in v) count(w,cj)]\n\t// =============================================================================\n\n\t//how many times this word has occurred in documents mapped to this category\n\tvar wordFrequencyCount = this.wordFrequencyCount[category][token] || 0; //count(wi,cj)\n\n\t//what is the count of all words that have ever been mapped to this category\n\tvar wordCount = this.wordCount[category]; //SUM[(for w in v) count(w,cj)\n\n\t//use laplace Add-1 Smoothing equation\n\t//=> ( P(wi|Cj) = count(wi,cj) + 1 ) / ( SUM[(for w in v) count(w,cj)] + |VocabSize| )\n\treturn ( wordFrequencyCount + 1 ) / ( wordCount + this.getVocabularySize() );\n};\n\n/**\n * Determine what category `text` belongs to.\n * Use Laplace (add-1) smoothing to adjust for words that do not appear in our vocabulary (unknown words)\n *\n * @param  {String} text\n * @return {Object} cMAP and categories\n */\nNaiveBayesClassifier.prototype.categorize = function (text) {\n\tvar self = this,  //get reference to instance\n\t\t\tmaxProbability = -Infinity,\n\t\t\ttotalProbabilities = 0,\n\t\t\tcMAP = {}, //category of “maximum a posteriori” => most likely category\n\t\t\tcategoryProbabilities = {}; //probabilities of all categories\n\n\tvar tokens = self.tokenizer(text),\n\t\ttokenFrequencyTable = self.FrequencyTable(tokens);\n\n\tObject\n\t.keys(self.categories)\n\t.forEach(function(category) { //for each category, find the probability of the text belonging to it\n\t\tif (!self.categories[category]) { return; } //ignore categories that have been switched off\n\n\t\t// 1. Find overall probability of this category\n\t\t//=> P(Cj) = docCount(C=cj) / Ndoc\n\t\t// =============================================================================\n\t\t\n\t\t//Put of all documents we've ever looked at, how many were mapped to this category\n\t\tvar categoryProbability = self.docFrequencyCount[category] / self.totalNumberOfDocuments;\n\n\t\t//take the log to avoid underflow with large datasets - http://www.johndcook.com/blog/2012/07/26/avoiding-underflow-in-bayesian-computations/\n\t\tvar logCategoryProbability = Math.log(categoryProbability); //start with P(Cj), we will add P(wi|Cj) incrementally below\n\n\t\t// 2. Find probability of each word in this categogy\n\t\t//=> P(wi|Cj) = count(wi,cj) / SUM[(for w in v) count(w,cj)]\n\t\t// =============================================================================\n\n\t\tObject\n\t\t.keys(tokenFrequencyTable)\n\t\t.forEach(function(token) { //for each token in our token frequency table\n\n\t\t\t//determine the log of the probability of this token belonging to the current category\n\t\t\t//=> log( P(w|c) )\n\t\t\tvar tokenProbability = self.tokenProbability(token, category);\n\t\t\t//and add it to our running probability that the text belongs to the current category\n\t\t\tlogCategoryProbability += Math.log(tokenProbability); //TODO: look into *frequencyTable[token];\n\n\t\t\t// console.log('token: %s | category: `%s` | probability: %d', token, category, tokenProbability);\n\t\t});\n\n\t\t// 3. Find the most likely category, thus far...\n\t\t// =============================================================================\n\n\t\tcategoryProbability = Math.exp(logCategoryProbability); //reverse the log and get an actual value\n\t\ttotalProbabilities += categoryProbability; //calculate totals as we go, we'll use this to normalise later\n\n\t\tif (logCategoryProbability > maxProbability) { //find cMAP\n\t\t\tmaxProbability = logCategoryProbability;\n\t\t\tcMAP = {\n\t\t\t\tcategory: category,\n\t\t\t\tprobability: categoryProbability\n\t\t\t};\n\t\t}\n\n\t\tcategoryProbabilities[category] = categoryProbability;\n\t});\n\n\t//normalise (out of 1) the probabilities, so that they make a bit more sense to the average person\n\tObject\n\t.keys(categoryProbabilities)\n\t.forEach(function(category) {\n\t\tcategoryProbabilities[category] /= totalProbabilities;\n\t});\n\n\treturn {\n\t\tcategory: cMAP.category || 'unclassified',\n\t\tprobability: (cMAP.probability /= totalProbabilities) || -Infinity,\n\t\tcategories: categoryProbabilities\n\t};\n};"},"params":[{"title":"param","description":null,"type":{"type":"NameExpression","name":"String"},"name":"text"},{"title":"param","description":null,"type":{"type":"NameExpression","name":"String"},"name":"category"}],"returns":[{"title":"returns","description":"NaiveBayesClassifier","type":{"type":"NameExpression","name":"Object"}}],"name":"learn","kind":"function","memberof":"NaiveBayesClassifier","scope":"instance"}
,
{"description":"Calculate probability that a `token` belongs to a `category`","tags":[{"title":"param","description":null,"type":{"type":"NameExpression","name":"String"},"name":"token"},{"title":"param","description":null,"type":{"type":"NameExpression","name":"String"},"name":"category"},{"title":"returns","description":"probability","type":{"type":"NameExpression","name":"Number"}},{"title":"name","name":"tokenProbability"},{"title":"kind","kind":"function"},{"title":"memberof","description":"NaiveBayesClassifier"},{"title":"instance"}],"context":{"loc":{"start":{"line":203,"column":0},"end":{"line":216,"column":2}},"file":"/Users/hsalem/Developer/naive-bayes-classifier/src/NaiveBayesClassifier.js","code":"'use strict';\n\n/**\n * NaiveBayesClassifier constructor fuction. Takes an (optional) options object containing:\n *   - {Function} tokenizer => custom tokenization function\n *\n * @constructor\n * @param  {Object} options - Options to be used for intialisation\n * @return {Object} NaiveBayesClassifier\n */\nvar NaiveBayesClassifier = function(options) {\n\t// OPTIONS\n\t// =============================================================================\n\tthis.options = {};\n\n\tif (!options) {\n\t\tif (typeof options !== 'object') {\n\t\t\tthrow new TypeError('NaiveBayesClassifier got invalid `options`: `' + options + '`. Please pass in an object.');\n\t\t}\n\t\tthis.options = options;\n\t}\n\n\t// TOKENIZER\n\t// =============================================================================\n\t/**\n\t * Given an input string, tokenize it into an array of word tokens.\n\t * This tokenizer adopts a naive \"Bag of words\" assumption.\n\t * This is the default tokenization function used if the user does not provide one in `options`.\n\t *\n\t * @private\n\t * @param  {String} text\n\t * @return {Array}\n\t */\n\tvar defaultTokenizer = function(text) {\n\t\t//remove punctuation from text (anything that isn't a word char or a space), and enforce lowercase\n\t\tvar rgxPunctuation = new RegExp(/[^\\w\\s]/g);\n\t\tvar sanitized = text.replace(rgxPunctuation, ' ').toLowerCase();\n\n\t\treturn sanitized.split(/\\s+/);\n\t};\n\n\tthis.tokenizer = this.options.tokenizer || defaultTokenizer;\n\t\n\t// VOCABULARY\n\t// =============================================================================\n\t//initialize our vocabulary and its size\n\tthis.vocabulary = {};\n\tthis.vocabularySize = 0;\n\n\t// CATEGORIES - hashmap of our category names\n\t// =============================================================================\n\tthis.categories = {};\n\n\t// PRIOR PROBABILITY: P(Cj) = docCount(C=cj) / Ndoc\n\t// =============================================================================\n\n\t//document frequency table for each of our categories\n\t//=> for each category, how often were documents mapped to it\n\tthis.docFrequencyCount = {}; //docCount(class)\n\n\tthis.totalNumberOfDocuments = 0; //Ndoc => number of documents we have learned from\n\n\t// LIKELIHOOD: P(wi|Cj) = count(wi,cj) / SUM[(for w in v) count(w,cj)]\n\t// =============================================================================\n\n\t//word frequency table for each of our categories\n\t//=> for each category, how frequent was a given word mapped to it\n\tthis.wordFrequencyCount = {}; //count(wi,cj)\n\n\t//word count table for each of our categories\n\t//=> for each category, how many words in total were mapped to it\n\tthis.wordCount = {}; //SUM[(for w in v) count(w,cj)\n};\n\n/**\n * Library version number\n */\nNaiveBayesClassifier.VERSION = '0.1.0'; // current version | Note: JS Functions are first class Objects\n\nNaiveBayesClassifier.withClassifier = function(classifier) {\n\treturn new NaiveBayesClassifier(classifier.options);\n};\n\nNaiveBayesClassifier.prototype.addWordToVocabulary = function(word) {\n\tif (!this.vocabulary[word]) {\n\t\tthis.vocabulary[word] = true;\n\t\tthis.vocabularySize += 1;\n\t}\n};\nNaiveBayesClassifier.prototype.getVocabularySize = function() {\n\tthis.vocabularySize = Object.keys(this.vocabulary).length;\n\treturn this.vocabularySize;\n};\n\n/**\n * Initialize each of our data structure entries for this new category\n *\n * @param  {String} categoryName - Name of the category you want to get or create\n * @return {String} category\n */\nNaiveBayesClassifier.prototype.getOrCreateCategory = function(categoryName) {\n\tif (!categoryName || typeof categoryName !== 'string') {\n\t\tthrow new TypeError('Category creator got invalid category name: `' + categoryName + '`. Please pass in a String.');\n\t}\n\n\t//simple singleton for each category\n\tif (!this.categories[categoryName]) {\n\t\t//setup counters\n\t\tthis.docFrequencyCount[categoryName] = 0;\n\t\tthis.wordFrequencyCount[categoryName] = {};\n\t\tthis.wordCount[categoryName] = 0;\n\n\t\t//add new category to our list\n\t\tthis.categories[categoryName] = true;\n\t}\n\treturn this.categories[categoryName] ? categoryName : undefined;\n};\n\n/**\n * Build a frequency hashmap where\n * - the keys are the entries in `tokens`\n * - the values are the frequency of each entry (`tokens`)\n *\n * @param  {Array} tokens - Normalized word array\n * @return {Object} FrequencyTable\n */\nNaiveBayesClassifier.prototype.FrequencyTable = function(tokens) {\n\tvar frequencyTable = {};\n\n\ttokens.forEach(function (token) {\n\t\tif (!frequencyTable[token]) {\n\t\t\tfrequencyTable[token] = 1;\n\t\t} else {\n\t\t\tfrequencyTable[token] += 1;\n\t\t}\n\t});\n\n\treturn frequencyTable;\n};\n\n/**\n * Train our naive-bayes classifier by telling it what `category` some `text` corresponds to.\n *\n * @param  {String} text\n * @param  {String} category\n * @return {Object} NaiveBayesClassifier\n */\nNaiveBayesClassifier.prototype.learn = function(text, category) {\n\tvar self = this; //get reference to instance\n\n\tcategory = self.Category(category); //get or create a category\n\n\tself.docFrequencyCount[category] += 1; //update our count of how many documents mapped to this category\n\tself.totalNumberOfDocuments += 1; //update the total number of documents we have learned from\n\n\tvar tokens = self.tokenizer(text); //break up the text into tokens\n\tvar tokenFrequencyTable = self.FrequencyTable(tokens); //get a frequency count for each token in the text\n\n\t// Update our vocabulary and our word frequency counts for this category\n\t// =============================================================================\n\tObject\n\t.keys(tokenFrequencyTable)\n\t.forEach(function(token) { //for each token in our tokenFrequencyTable\n\t\t\n\t\tself.addWordToVocabulary(token); //add this word to our vocabulary if not already existing\n\n\t\tvar frequencyOfTokenInText = tokenFrequencyTable[token]; //look it up once, for speed\n\n\t\t//update the frequency information for this word in this category\n\t\tif (!self.wordFrequencyCount[category][token]) {\n\t\t\tself.wordFrequencyCount[category][token] = frequencyOfTokenInText; //set it for the first time\n\t\t} else {\n\t\t\tself.wordFrequencyCount[category][token] += frequencyOfTokenInText; //add to what's already there in the count\n\t\t}\n\n\t\tself.wordCount[category] += frequencyOfTokenInText; //add to the count of all words we have seen mapped to this category\n\t});\n\n\treturn self;\n};\n\n/**\n * Calculate probability that a `token` belongs to a `category`\n *\n * @param  {String} token\n * @param  {String} category\n * @return {Number} probability\n */\nNaiveBayesClassifier.prototype.tokenProbability = function(token, category) {\n\t// Recall => P(wi|Cj) = count(wi,cj) / SUM[(for w in v) count(w,cj)]\n\t// =============================================================================\n\n\t//how many times this word has occurred in documents mapped to this category\n\tvar wordFrequencyCount = this.wordFrequencyCount[category][token] || 0; //count(wi,cj)\n\n\t//what is the count of all words that have ever been mapped to this category\n\tvar wordCount = this.wordCount[category]; //SUM[(for w in v) count(w,cj)\n\n\t//use laplace Add-1 Smoothing equation\n\t//=> ( P(wi|Cj) = count(wi,cj) + 1 ) / ( SUM[(for w in v) count(w,cj)] + |VocabSize| )\n\treturn ( wordFrequencyCount + 1 ) / ( wordCount + this.getVocabularySize() );\n};\n\n/**\n * Determine what category `text` belongs to.\n * Use Laplace (add-1) smoothing to adjust for words that do not appear in our vocabulary (unknown words)\n *\n * @param  {String} text\n * @return {Object} cMAP and categories\n */\nNaiveBayesClassifier.prototype.categorize = function (text) {\n\tvar self = this,  //get reference to instance\n\t\t\tmaxProbability = -Infinity,\n\t\t\ttotalProbabilities = 0,\n\t\t\tcMAP = {}, //category of “maximum a posteriori” => most likely category\n\t\t\tcategoryProbabilities = {}; //probabilities of all categories\n\n\tvar tokens = self.tokenizer(text),\n\t\ttokenFrequencyTable = self.FrequencyTable(tokens);\n\n\tObject\n\t.keys(self.categories)\n\t.forEach(function(category) { //for each category, find the probability of the text belonging to it\n\t\tif (!self.categories[category]) { return; } //ignore categories that have been switched off\n\n\t\t// 1. Find overall probability of this category\n\t\t//=> P(Cj) = docCount(C=cj) / Ndoc\n\t\t// =============================================================================\n\t\t\n\t\t//Put of all documents we've ever looked at, how many were mapped to this category\n\t\tvar categoryProbability = self.docFrequencyCount[category] / self.totalNumberOfDocuments;\n\n\t\t//take the log to avoid underflow with large datasets - http://www.johndcook.com/blog/2012/07/26/avoiding-underflow-in-bayesian-computations/\n\t\tvar logCategoryProbability = Math.log(categoryProbability); //start with P(Cj), we will add P(wi|Cj) incrementally below\n\n\t\t// 2. Find probability of each word in this categogy\n\t\t//=> P(wi|Cj) = count(wi,cj) / SUM[(for w in v) count(w,cj)]\n\t\t// =============================================================================\n\n\t\tObject\n\t\t.keys(tokenFrequencyTable)\n\t\t.forEach(function(token) { //for each token in our token frequency table\n\n\t\t\t//determine the log of the probability of this token belonging to the current category\n\t\t\t//=> log( P(w|c) )\n\t\t\tvar tokenProbability = self.tokenProbability(token, category);\n\t\t\t//and add it to our running probability that the text belongs to the current category\n\t\t\tlogCategoryProbability += Math.log(tokenProbability); //TODO: look into *frequencyTable[token];\n\n\t\t\t// console.log('token: %s | category: `%s` | probability: %d', token, category, tokenProbability);\n\t\t});\n\n\t\t// 3. Find the most likely category, thus far...\n\t\t// =============================================================================\n\n\t\tcategoryProbability = Math.exp(logCategoryProbability); //reverse the log and get an actual value\n\t\ttotalProbabilities += categoryProbability; //calculate totals as we go, we'll use this to normalise later\n\n\t\tif (logCategoryProbability > maxProbability) { //find cMAP\n\t\t\tmaxProbability = logCategoryProbability;\n\t\t\tcMAP = {\n\t\t\t\tcategory: category,\n\t\t\t\tprobability: categoryProbability\n\t\t\t};\n\t\t}\n\n\t\tcategoryProbabilities[category] = categoryProbability;\n\t});\n\n\t//normalise (out of 1) the probabilities, so that they make a bit more sense to the average person\n\tObject\n\t.keys(categoryProbabilities)\n\t.forEach(function(category) {\n\t\tcategoryProbabilities[category] /= totalProbabilities;\n\t});\n\n\treturn {\n\t\tcategory: cMAP.category || 'unclassified',\n\t\tprobability: (cMAP.probability /= totalProbabilities) || -Infinity,\n\t\tcategories: categoryProbabilities\n\t};\n};"},"params":[{"title":"param","description":null,"type":{"type":"NameExpression","name":"String"},"name":"token"},{"title":"param","description":null,"type":{"type":"NameExpression","name":"String"},"name":"category"}],"returns":[{"title":"returns","description":"probability","type":{"type":"NameExpression","name":"Number"}}],"name":"tokenProbability","kind":"function","memberof":"NaiveBayesClassifier","scope":"instance"}
]
